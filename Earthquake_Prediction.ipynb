{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFIOZNdkZzzi1xBIow0/hw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahavi-r31/Comprehensive-Analysis-of-Earthquake-Patterns-and-Trends-in-India/blob/main/Earthquake_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gaussian Mixture Models (GMM) for Classification:\n",
        "Gaussian Mixture Models can be used for clustering or classification tasks. However, in the case of earthquake prediction, they might be used for clustering seismic activity into different categories or clusters."
      ],
      "metadata": {
        "id": "nOR827nZDLIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "earthquake_data = pd.read_csv('/content/Earthquake.csv')\n",
        "\n",
        "# Selecting relevant features for prediction\n",
        "selected_features = ['time', 'latitude', 'longitude', 'depth', 'mag']\n",
        "\n",
        "# Considering only the selected features\n",
        "data = earthquake_data[selected_features]\n",
        "\n",
        "# Handling missing values if any\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert time attribute to datetime format\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "\n",
        "# Extract useful features from time\n",
        "data['year'] = data['time'].dt.year\n",
        "data['month'] = data['time'].dt.month\n",
        "data['day'] = data['time'].dt.day\n",
        "data['hour'] = data['time'].dt.hour\n",
        "data['minute'] = data['time'].dt.minute\n",
        "data['second'] = data['time'].dt.second\n",
        "\n",
        "# Drop the original 'time' column as we have extracted useful features\n",
        "data = data.drop('time', axis=1)\n",
        "\n",
        "# Select features for clustering\n",
        "X = data[['year', 'month', 'day', 'hour', 'minute', 'second', 'latitude', 'longitude', 'depth']]\n",
        "\n",
        "# Initializing and fitting Gaussian Mixture Model for clustering\n",
        "gmm = GaussianMixture(n_components=5, random_state=42)  # Specify the number of components/clusters\n",
        "clusters = gmm.fit_predict(X)\n",
        "\n",
        "# Add clusters to the dataset for analysis or further use\n",
        "data['cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "QgQirReIDIK7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hidden Markov Models (HMM):\n",
        "Hidden Markov Models are effective for modelling temporal sequences, making them suitable for capturing patterns in seismic activity over time."
      ],
      "metadata": {
        "id": "7i4WtFVRDVHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dA2WfjFG08i",
        "outputId": "67727c65-f35e-4d63-9e59-f4dc40bba257"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/160.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/160.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.2.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn import hmm\n",
        "\n",
        "# Assuming 'data' is your pre-processed dataset with relevant attributes\n",
        "\n",
        "# Prepare data in a sequence format suitable for HMM\n",
        "# (Needs further pre-processing and structuring data into sequences)\n",
        "\n",
        "# Create an HMM model instance\n",
        "model = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n",
        "\n",
        "# Train the HMM model\n",
        "model.fit(data)  # 'data' should be in a suitable sequence format\n",
        "\n",
        "# Predict using the trained HMM model\n",
        "predictions = model.predict(data)  # Use 'data' or test data accordingly\n",
        "\n",
        "# Further analysis and evaluation of HMM predictions\n",
        "# (Evaluation metrics might differ based on the specific use case and problem)\n"
      ],
      "metadata": {
        "id": "bNdrfLkDDgi9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost:\n",
        "XGBoost is a powerful gradient boosting algorithm that handles complex feature interactions, making it suitable for earthquake prediction tasks."
      ],
      "metadata": {
        "id": "Goz9PVRzDkFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming 'data' is your pre-processed dataset with relevant attributes\n",
        "\n",
        "# Splitting data into features and target variable\n",
        "X = data[['time', 'latitude', 'longitude', 'depth']]\n",
        "y = data['mag']  # Assuming 'mag' is the target variable for magnitude prediction\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating and training the XGBoost regression model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluating the model (example using Mean Squared Error for regression)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "sruWgkO7DqRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}